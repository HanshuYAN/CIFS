

------ ******* ------ New Log ------ ******* ------
Namespace(GPU_IDs=[0], SEED=0, attack_loss='Joint', beta_atk=2.0, beta_cls=2, checkpoint='./experiments/Cln_Res18_cifar10_CIFS/nets/ckp_best.pt', exp_path='./experiments/PAT_Res18_cifar10_CIFS_softmax2', is_AdvTr=True, is_Train=True, lr=0.1, milestones=[75, 90], momentum=0.9, net_only=True, network='CIFS_L4', resume=True, test_batch_size=250, tr_epochs=120, train_batch_size=128, weight_decay=0.0002)
ResNet_L4(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): ModuleList(
    (0): CIFSBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (Probe): CIFS(
        (Probe): Sequential(
          (0): nlBlock(
            (global_avg_flatten): Global_Avg_Flatten()
            (fc): Linear(in_features=512, out_features=128, bias=True)
            (softplus): Softplus(beta=5, threshold=20)
          )
          (1): Linear(in_features=128, out_features=10, bias=True)
        )
      )
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): CIFSBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (Probe): CIFS(
        (Probe): Sequential(
          (0): Global_Avg_Flatten()
          (1): Linear(in_features=512, out_features=10, bias=True)
        )
      )
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=512, out_features=10, bias=True)
)
Training Epoch: 0; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 3.856005, Time for Batches: 41.054018
[25472/50000 (51%)], Loss: 3.482254, Time for Batches: 40.915025
[38272/50000 (76%)], Loss: 2.867261, Time for Batches: 41.226632
***** Test set acc: 37.57%, adv: 23.39%.	 Time for an epoch: 251.66
Best result @ 000, 0.2339 

Training Epoch: 1; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.712894, Time for Batches: 41.526889
[25472/50000 (51%)], Loss: 2.376369, Time for Batches: 41.712921
[38272/50000 (76%)], Loss: 2.657995, Time for Batches: 41.807856
- evaluation skipped!	 Time for an epoch: 165.25
Best result @ 000, 0.2339 

Training Epoch: 2; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.732682, Time for Batches: 41.977116
[25472/50000 (51%)], Loss: 2.237720, Time for Batches: 41.961181
[38272/50000 (76%)], Loss: 2.284838, Time for Batches: 41.993334
- evaluation skipped!	 Time for an epoch: 165.20
Best result @ 000, 0.2339 

Training Epoch: 3; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.176611, Time for Batches: 42.098064
[25472/50000 (51%)], Loss: 2.366873, Time for Batches: 41.982676
[38272/50000 (76%)], Loss: 2.398073, Time for Batches: 41.992154
- evaluation skipped!	 Time for an epoch: 165.32
Best result @ 000, 0.2339 

Training Epoch: 4; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.443301, Time for Batches: 42.074098
[25472/50000 (51%)], Loss: 2.006442, Time for Batches: 41.944543
[38272/50000 (76%)], Loss: 2.059225, Time for Batches: 41.912585
- evaluation skipped!	 Time for an epoch: 165.21
Best result @ 000, 0.2339 

Training Epoch: 5; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.952322, Time for Batches: 42.027898
[25472/50000 (51%)], Loss: 2.123428, Time for Batches: 41.913165
[38272/50000 (76%)], Loss: 2.227536, Time for Batches: 41.843133
- evaluation skipped!	 Time for an epoch: 164.85
Best result @ 000, 0.2339 

Training Epoch: 6; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.093452, Time for Batches: 42.040195
[25472/50000 (51%)], Loss: 1.801828, Time for Batches: 41.971683
[38272/50000 (76%)], Loss: 2.204490, Time for Batches: 41.901852
- evaluation skipped!	 Time for an epoch: 165.07
Best result @ 000, 0.2339 

Training Epoch: 7; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.906262, Time for Batches: 42.056082
[25472/50000 (51%)], Loss: 2.043081, Time for Batches: 41.856639
[38272/50000 (76%)], Loss: 1.949582, Time for Batches: 41.933202
- evaluation skipped!	 Time for an epoch: 165.04
Best result @ 000, 0.2339 

Training Epoch: 8; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.087188, Time for Batches: 42.008858
[25472/50000 (51%)], Loss: 1.915126, Time for Batches: 41.932113
[38272/50000 (76%)], Loss: 1.636203, Time for Batches: 41.908704
- evaluation skipped!	 Time for an epoch: 165.09
Best result @ 000, 0.2339 

Training Epoch: 9; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.849575, Time for Batches: 41.923226
[25472/50000 (51%)], Loss: 1.800903, Time for Batches: 41.894637
[38272/50000 (76%)], Loss: 1.680911, Time for Batches: 41.955799
***** Test set acc: 66.65%, adv: 38.70%.	 Time for an epoch: 257.07
Best result @ 009, 0.387 

Training Epoch: 10; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.725951, Time for Batches: 41.892725
[25472/50000 (51%)], Loss: 1.855835, Time for Batches: 41.788675
[38272/50000 (76%)], Loss: 2.060673, Time for Batches: 41.892236
- evaluation skipped!	 Time for an epoch: 165.81
Best result @ 009, 0.387 

Training Epoch: 11; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.826288, Time for Batches: 42.014557
[25472/50000 (51%)], Loss: 1.815322, Time for Batches: 41.909985
[38272/50000 (76%)], Loss: 2.110366, Time for Batches: 42.040069
- evaluation skipped!	 Time for an epoch: 165.18
Best result @ 009, 0.387 

Training Epoch: 12; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.952093, Time for Batches: 42.078798
[25472/50000 (51%)], Loss: 1.761114, Time for Batches: 41.922309
[38272/50000 (76%)], Loss: 1.921188, Time for Batches: 41.876370
- evaluation skipped!	 Time for an epoch: 164.99
Best result @ 009, 0.387 

Training Epoch: 13; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.793427, Time for Batches: 42.048743
[25472/50000 (51%)], Loss: 1.844377, Time for Batches: 41.901804
[38272/50000 (76%)], Loss: 1.791472, Time for Batches: 41.899969
- evaluation skipped!	 Time for an epoch: 165.10
Best result @ 009, 0.387 

Training Epoch: 14; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.595356, Time for Batches: 42.097467
[25472/50000 (51%)], Loss: 1.920349, Time for Batches: 41.959860
[38272/50000 (76%)], Loss: 1.794920, Time for Batches: 41.957836
- evaluation skipped!	 Time for an epoch: 165.20
Best result @ 009, 0.387 

Training Epoch: 15; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.798514, Time for Batches: 41.927658
[25472/50000 (51%)], Loss: 1.879023, Time for Batches: 41.959792
[38272/50000 (76%)], Loss: 1.873137, Time for Batches: 41.914877
- evaluation skipped!	 Time for an epoch: 164.91
Best result @ 009, 0.387 

Training Epoch: 16; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.908626, Time for Batches: 42.067189
[25472/50000 (51%)], Loss: 1.866509, Time for Batches: 41.955995
[38272/50000 (76%)], Loss: 1.516295, Time for Batches: 41.937264
- evaluation skipped!	 Time for an epoch: 165.23
Best result @ 009, 0.387 

Training Epoch: 17; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.699407, Time for Batches: 42.087150
[25472/50000 (51%)], Loss: 1.782791, Time for Batches: 41.942427
[38272/50000 (76%)], Loss: 1.724664, Time for Batches: 41.982434
- evaluation skipped!	 Time for an epoch: 165.21
Best result @ 009, 0.387 

Training Epoch: 18; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.747807, Time for Batches: 42.057919
[25472/50000 (51%)], Loss: 1.928443, Time for Batches: 41.952508
[38272/50000 (76%)], Loss: 1.719691, Time for Batches: 41.904999
- evaluation skipped!	 Time for an epoch: 165.09
Best result @ 009, 0.387 

Training Epoch: 19; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.690884, Time for Batches: 42.024074
[25472/50000 (51%)], Loss: 1.706971, Time for Batches: 41.935820
[38272/50000 (76%)], Loss: 1.969331, Time for Batches: 41.933135
***** Test set acc: 65.72%, adv: 40.20%.	 Time for an epoch: 257.02
Best result @ 019, 0.402 

Training Epoch: 20; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.759061, Time for Batches: 41.742734
[25472/50000 (51%)], Loss: 1.804858, Time for Batches: 41.631639
[38272/50000 (76%)], Loss: 1.887288, Time for Batches: 41.864415
- evaluation skipped!	 Time for an epoch: 165.74
Best result @ 019, 0.402 

Training Epoch: 21; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.585105, Time for Batches: 41.773738
[25472/50000 (51%)], Loss: 1.625937, Time for Batches: 41.637670
[38272/50000 (76%)], Loss: 1.860246, Time for Batches: 41.875495
- evaluation skipped!	 Time for an epoch: 164.41
Best result @ 019, 0.402 

Training Epoch: 22; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.816078, Time for Batches: 42.057844
[25472/50000 (51%)], Loss: 1.764188, Time for Batches: 41.862482
[38272/50000 (76%)], Loss: 1.642619, Time for Batches: 41.711089
- evaluation skipped!	 Time for an epoch: 164.77
Best result @ 019, 0.402 

Training Epoch: 23; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.648466, Time for Batches: 41.852476
[25472/50000 (51%)], Loss: 1.527245, Time for Batches: 41.959351
[38272/50000 (76%)], Loss: 1.584080, Time for Batches: 41.655710
- evaluation skipped!	 Time for an epoch: 164.34
Best result @ 019, 0.402 

Training Epoch: 24; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.676888, Time for Batches: 41.795878
[25472/50000 (51%)], Loss: 1.645431, Time for Batches: 41.832294
[38272/50000 (76%)], Loss: 1.719459, Time for Batches: 41.968187
- evaluation skipped!	 Time for an epoch: 164.75
Best result @ 019, 0.402 

Training Epoch: 25; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.532998, Time for Batches: 41.998067
[25472/50000 (51%)], Loss: 1.523862, Time for Batches: 41.685108
[38272/50000 (76%)], Loss: 1.460434, Time for Batches: 41.650690
- evaluation skipped!	 Time for an epoch: 164.59
Best result @ 019, 0.402 

Training Epoch: 26; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.807051, Time for Batches: 41.986307
[25472/50000 (51%)], Loss: 1.656170, Time for Batches: 41.867318
[38272/50000 (76%)], Loss: 1.901850, Time for Batches: 41.920579
- evaluation skipped!	 Time for an epoch: 165.00
Best result @ 019, 0.402 

Training Epoch: 27; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.692316, Time for Batches: 42.011237
[25472/50000 (51%)], Loss: 1.722254, Time for Batches: 41.657622
[38272/50000 (76%)], Loss: 1.799902, Time for Batches: 41.868866
- evaluation skipped!	 Time for an epoch: 164.65
Best result @ 019, 0.402 

Training Epoch: 28; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.512514, Time for Batches: 41.828900
[25472/50000 (51%)], Loss: 1.603085, Time for Batches: 41.847141
[38272/50000 (76%)], Loss: 1.837313, Time for Batches: 41.920463
- evaluation skipped!	 Time for an epoch: 164.89
Best result @ 019, 0.402 

Training Epoch: 29; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.900231, Time for Batches: 42.002318
[25472/50000 (51%)], Loss: 1.634389, Time for Batches: 41.982907
[38272/50000 (76%)], Loss: 1.703306, Time for Batches: 41.921518
***** Test set acc: 71.29%, adv: 43.14%.	 Time for an epoch: 257.04
Best result @ 029, 0.4314 

Training Epoch: 30; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.487704, Time for Batches: 41.927944
[25472/50000 (51%)], Loss: 1.600326, Time for Batches: 41.926996
[38272/50000 (76%)], Loss: 1.877668, Time for Batches: 41.918922
- evaluation skipped!	 Time for an epoch: 165.99
Best result @ 029, 0.4314 

Training Epoch: 31; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.634438, Time for Batches: 42.065912
[25472/50000 (51%)], Loss: 1.448460, Time for Batches: 41.868452
[38272/50000 (76%)], Loss: 1.554783, Time for Batches: 41.909730
- evaluation skipped!	 Time for an epoch: 165.00
Best result @ 029, 0.4314 

Training Epoch: 32; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.665892, Time for Batches: 42.080722
[25472/50000 (51%)], Loss: 1.673208, Time for Batches: 41.760151
[38272/50000 (76%)], Loss: 1.715220, Time for Batches: 41.662112
- evaluation skipped!	 Time for an epoch: 164.53
Best result @ 029, 0.4314 

Training Epoch: 33; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.579573, Time for Batches: 41.851889
[25472/50000 (51%)], Loss: 1.768625, Time for Batches: 41.944220
[38272/50000 (76%)], Loss: 1.584734, Time for Batches: 41.889030
- evaluation skipped!	 Time for an epoch: 164.77
Best result @ 029, 0.4314 

Training Epoch: 34; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.847451, Time for Batches: 42.025087
[25472/50000 (51%)], Loss: 1.526101, Time for Batches: 41.978057
[38272/50000 (76%)], Loss: 1.443430, Time for Batches: 41.814321
- evaluation skipped!	 Time for an epoch: 164.89
Best result @ 029, 0.4314 

Training Epoch: 35; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.688294, Time for Batches: 41.916655
[25472/50000 (51%)], Loss: 1.626507, Time for Batches: 41.724392
[38272/50000 (76%)], Loss: 1.562315, Time for Batches: 41.863559
- evaluation skipped!	 Time for an epoch: 164.59
Best result @ 029, 0.4314 

Training Epoch: 36; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.560357, Time for Batches: 42.100967
[25472/50000 (51%)], Loss: 1.654476, Time for Batches: 41.944232
[38272/50000 (76%)], Loss: 1.534833, Time for Batches: 41.976988
- evaluation skipped!	 Time for an epoch: 165.19
Best result @ 029, 0.4314 

Training Epoch: 37; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.732642, Time for Batches: 42.057485
[25472/50000 (51%)], Loss: 1.649797, Time for Batches: 41.892051
[38272/50000 (76%)], Loss: 1.505331, Time for Batches: 41.807859
- evaluation skipped!	 Time for an epoch: 164.74
Best result @ 029, 0.4314 

Training Epoch: 38; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.526504, Time for Batches: 41.887034
[25472/50000 (51%)], Loss: 1.722343, Time for Batches: 41.916627
[38272/50000 (76%)], Loss: 1.491673, Time for Batches: 41.863523
- evaluation skipped!	 Time for an epoch: 164.83
Best result @ 029, 0.4314 

Training Epoch: 39; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.727126, Time for Batches: 42.004869
[25472/50000 (51%)], Loss: 1.653453, Time for Batches: 41.911305
[38272/50000 (76%)], Loss: 1.732719, Time for Batches: 41.898835
***** Test set acc: 68.99%, adv: 40.66%.	 Time for an epoch: 257.39
Best result @ 029, 0.4314 

Training Epoch: 40; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.481295, Time for Batches: 41.956042
[25472/50000 (51%)], Loss: 1.561295, Time for Batches: 41.901744
[38272/50000 (76%)], Loss: 1.541341, Time for Batches: 41.943405
- evaluation skipped!	 Time for an epoch: 164.94
Best result @ 029, 0.4314 

Training Epoch: 41; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.489016, Time for Batches: 42.019117
[25472/50000 (51%)], Loss: 1.714504, Time for Batches: 41.868525
[38272/50000 (76%)], Loss: 1.836631, Time for Batches: 41.890934
- evaluation skipped!	 Time for an epoch: 165.04
Best result @ 029, 0.4314 

Training Epoch: 42; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.533309, Time for Batches: 41.905426
[25472/50000 (51%)], Loss: 1.742908, Time for Batches: 41.828830
[38272/50000 (76%)], Loss: 1.623976, Time for Batches: 41.833775
- evaluation skipped!	 Time for an epoch: 164.72
Best result @ 029, 0.4314 

Training Epoch: 43; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.442476, Time for Batches: 41.966017
[25472/50000 (51%)], Loss: 1.582856, Time for Batches: 41.911522
[38272/50000 (76%)], Loss: 1.614187, Time for Batches: 42.014215
- evaluation skipped!	 Time for an epoch: 165.07
Best result @ 029, 0.4314 

Training Epoch: 44; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.725657, Time for Batches: 42.071313
[25472/50000 (51%)], Loss: 1.673014, Time for Batches: 42.013030
[38272/50000 (76%)], Loss: 1.640625, Time for Batches: 41.883643
- evaluation skipped!	 Time for an epoch: 164.94
Best result @ 029, 0.4314 

Training Epoch: 45; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.669616, Time for Batches: 41.897065
[25472/50000 (51%)], Loss: 1.477033, Time for Batches: 41.886928
[38272/50000 (76%)], Loss: 1.648606, Time for Batches: 41.933237
- evaluation skipped!	 Time for an epoch: 164.88
Best result @ 029, 0.4314 

Training Epoch: 46; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.801450, Time for Batches: 42.053214
[25472/50000 (51%)], Loss: 1.522788, Time for Batches: 41.884324
[38272/50000 (76%)], Loss: 1.395040, Time for Batches: 41.917774
- evaluation skipped!	 Time for an epoch: 164.99
Best result @ 029, 0.4314 

Training Epoch: 47; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.568408, Time for Batches: 41.919436
[25472/50000 (51%)], Loss: 1.475001, Time for Batches: 41.687328
[38272/50000 (76%)], Loss: 1.832571, Time for Batches: 41.565375
- evaluation skipped!	 Time for an epoch: 164.32
Best result @ 029, 0.4314 

Training Epoch: 48; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.760128, Time for Batches: 41.738600
[25472/50000 (51%)], Loss: 1.623963, Time for Batches: 41.740383
[38272/50000 (76%)], Loss: 1.486420, Time for Batches: 42.009101
- evaluation skipped!	 Time for an epoch: 164.65
Best result @ 029, 0.4314 

Training Epoch: 49; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.587670, Time for Batches: 42.019357
[25472/50000 (51%)], Loss: 1.836140, Time for Batches: 41.953762
[38272/50000 (76%)], Loss: 1.421770, Time for Batches: 41.956211
***** Test set acc: 69.03%, adv: 44.17%.	 Time for an epoch: 257.12
Best result @ 049, 0.4417 

Training Epoch: 50; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.667135, Time for Batches: 41.698033
[25472/50000 (51%)], Loss: 1.533770, Time for Batches: 41.800897
[38272/50000 (76%)], Loss: 1.510608, Time for Batches: 41.954854
- evaluation skipped!	 Time for an epoch: 166.80
Best result @ 049, 0.4417 

Training Epoch: 51; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.668962, Time for Batches: 42.037630
[25472/50000 (51%)], Loss: 1.503926, Time for Batches: 41.936819
[38272/50000 (76%)], Loss: 1.642250, Time for Batches: 41.890118
***** Test set acc: 72.81%, adv: 44.60%.	 Time for an epoch: 257.07
Best result @ 051, 0.446 

Training Epoch: 52; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.535436, Time for Batches: 41.913326
[25472/50000 (51%)], Loss: 1.408192, Time for Batches: 41.911242
[38272/50000 (76%)], Loss: 1.833088, Time for Batches: 41.975521
- evaluation skipped!	 Time for an epoch: 167.13
Best result @ 051, 0.446 

Training Epoch: 53; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.484546, Time for Batches: 41.949210
[25472/50000 (51%)], Loss: 1.557972, Time for Batches: 41.904861
[38272/50000 (76%)], Loss: 1.451163, Time for Batches: 41.917671
***** Test set acc: 72.14%, adv: 39.19%.	 Time for an epoch: 257.13
Best result @ 051, 0.446 

Training Epoch: 54; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.530251, Time for Batches: 41.879906
[25472/50000 (51%)], Loss: 1.695490, Time for Batches: 41.856562
[38272/50000 (76%)], Loss: 1.776908, Time for Batches: 41.800991
- evaluation skipped!	 Time for an epoch: 165.95
Best result @ 051, 0.446 

Training Epoch: 55; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.273044, Time for Batches: 41.914535
[25472/50000 (51%)], Loss: 1.561508, Time for Batches: 41.903293
[38272/50000 (76%)], Loss: 1.484273, Time for Batches: 41.913087
***** Test set acc: 73.59%, adv: 42.98%.	 Time for an epoch: 256.75
Best result @ 051, 0.446 

Training Epoch: 56; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.936509, Time for Batches: 42.016938
[25472/50000 (51%)], Loss: 1.497254, Time for Batches: 41.931980
[38272/50000 (76%)], Loss: 1.655803, Time for Batches: 41.956578
- evaluation skipped!	 Time for an epoch: 166.07
Best result @ 051, 0.446 

Training Epoch: 57; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.423365, Time for Batches: 42.038256
[25472/50000 (51%)], Loss: 1.656256, Time for Batches: 41.913419
[38272/50000 (76%)], Loss: 1.660390, Time for Batches: 41.932608
***** Test set acc: 73.67%, adv: 44.01%.	 Time for an epoch: 257.08
Best result @ 051, 0.446 

Training Epoch: 58; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.814216, Time for Batches: 42.014676
[25472/50000 (51%)], Loss: 1.498196, Time for Batches: 41.951437
[38272/50000 (76%)], Loss: 1.555151, Time for Batches: 41.891376
- evaluation skipped!	 Time for an epoch: 166.10
Best result @ 051, 0.446 

Training Epoch: 59; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.272236, Time for Batches: 42.008033
[25472/50000 (51%)], Loss: 1.752452, Time for Batches: 41.938699
[38272/50000 (76%)], Loss: 1.549686, Time for Batches: 41.877426
***** Test set acc: 72.40%, adv: 41.33%.	 Time for an epoch: 256.91
Best result @ 051, 0.446 

Training Epoch: 60; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.435944, Time for Batches: 41.997746
[25472/50000 (51%)], Loss: 1.482025, Time for Batches: 41.938657
[38272/50000 (76%)], Loss: 1.552345, Time for Batches: 41.890205
- evaluation skipped!	 Time for an epoch: 166.05
Best result @ 051, 0.446 

Training Epoch: 61; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.764710, Time for Batches: 42.037697
[25472/50000 (51%)], Loss: 1.498757, Time for Batches: 41.929203
[38272/50000 (76%)], Loss: 1.527639, Time for Batches: 41.906934
***** Test set acc: 73.98%, adv: 43.91%.	 Time for an epoch: 256.96
Best result @ 051, 0.446 

Training Epoch: 62; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.386736, Time for Batches: 41.977235
[25472/50000 (51%)], Loss: 1.559639, Time for Batches: 41.835447
[38272/50000 (76%)], Loss: 1.741502, Time for Batches: 41.835396
- evaluation skipped!	 Time for an epoch: 165.81
Best result @ 051, 0.446 

Training Epoch: 63; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.587739, Time for Batches: 42.023018
[25472/50000 (51%)], Loss: 1.506119, Time for Batches: 41.935686
[38272/50000 (76%)], Loss: 1.749756, Time for Batches: 42.007959
***** Test set acc: 75.29%, adv: 43.86%.	 Time for an epoch: 257.15
Best result @ 051, 0.446 

Training Epoch: 64; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.427799, Time for Batches: 41.982136
[25472/50000 (51%)], Loss: 1.664930, Time for Batches: 41.958224
[38272/50000 (76%)], Loss: 1.382703, Time for Batches: 41.968254
- evaluation skipped!	 Time for an epoch: 166.62
Best result @ 051, 0.446 

Training Epoch: 65; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.529691, Time for Batches: 42.027961
[25472/50000 (51%)], Loss: 1.323426, Time for Batches: 41.943269
[38272/50000 (76%)], Loss: 1.802483, Time for Batches: 41.892783
***** Test set acc: 72.69%, adv: 44.22%.	 Time for an epoch: 257.39
Best result @ 051, 0.446 

Training Epoch: 66; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.609899, Time for Batches: 41.681457
[25472/50000 (51%)], Loss: 1.390446, Time for Batches: 41.924489
[38272/50000 (76%)], Loss: 1.733858, Time for Batches: 41.945803
- evaluation skipped!	 Time for an epoch: 166.98
Best result @ 051, 0.446 

Training Epoch: 67; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.414711, Time for Batches: 42.038242
[25472/50000 (51%)], Loss: 1.783012, Time for Batches: 41.926200
[38272/50000 (76%)], Loss: 1.598590, Time for Batches: 41.903525
***** Test set acc: 71.32%, adv: 43.27%.	 Time for an epoch: 257.15
Best result @ 051, 0.446 

Training Epoch: 68; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.443875, Time for Batches: 41.918410
[25472/50000 (51%)], Loss: 1.610399, Time for Batches: 41.784839
[38272/50000 (76%)], Loss: 1.807658, Time for Batches: 41.660149
- evaluation skipped!	 Time for an epoch: 165.58
Best result @ 051, 0.446 

Training Epoch: 69; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.335453, Time for Batches: 41.983591
[25472/50000 (51%)], Loss: 1.466646, Time for Batches: 41.872107
[38272/50000 (76%)], Loss: 1.405341, Time for Batches: 41.896244
***** Test set acc: 74.54%, adv: 43.72%.	 Time for an epoch: 256.99
Best result @ 051, 0.446 

Training Epoch: 70; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.323061, Time for Batches: 41.930419
[25472/50000 (51%)], Loss: 1.365544, Time for Batches: 41.957403
[38272/50000 (76%)], Loss: 1.415666, Time for Batches: 41.927302
- evaluation skipped!	 Time for an epoch: 166.05
Best result @ 051, 0.446 

Training Epoch: 71; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.472036, Time for Batches: 41.952101
[25472/50000 (51%)], Loss: 1.399829, Time for Batches: 41.900833
[38272/50000 (76%)], Loss: 1.393104, Time for Batches: 41.950388
***** Test set acc: 73.90%, adv: 43.21%.	 Time for an epoch: 257.06
Best result @ 051, 0.446 

Training Epoch: 72; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.485356, Time for Batches: 41.941367
[25472/50000 (51%)], Loss: 1.509462, Time for Batches: 41.869361
[38272/50000 (76%)], Loss: 1.472775, Time for Batches: 41.927480
- evaluation skipped!	 Time for an epoch: 166.00
Best result @ 051, 0.446 

Training Epoch: 73; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.389508, Time for Batches: 42.076729
[25472/50000 (51%)], Loss: 1.840457, Time for Batches: 41.915729
[38272/50000 (76%)], Loss: 1.681582, Time for Batches: 41.879641
***** Test set acc: 75.97%, adv: 43.22%.	 Time for an epoch: 257.04
Best result @ 051, 0.446 

Training Epoch: 74; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.554946, Time for Batches: 41.924994
[25472/50000 (51%)], Loss: 1.401062, Time for Batches: 42.012366
[38272/50000 (76%)], Loss: 1.523126, Time for Batches: 42.072195
- evaluation skipped!	 Time for an epoch: 166.27
Best result @ 051, 0.446 

Training Epoch: 75; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.115764, Time for Batches: 41.960913
[25472/50000 (51%)], Loss: 1.156851, Time for Batches: 41.908653
[38272/50000 (76%)], Loss: 1.028461, Time for Batches: 41.940464
***** Test set acc: 81.36%, adv: 50.18%.	 Time for an epoch: 257.36
Best result @ 075, 0.5018 

Training Epoch: 76; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.098430, Time for Batches: 41.824430
[25472/50000 (51%)], Loss: 1.047521, Time for Batches: 41.964245
[38272/50000 (76%)], Loss: 0.989325, Time for Batches: 41.911714
- evaluation skipped!	 Time for an epoch: 167.01
Best result @ 075, 0.5018 

Training Epoch: 77; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.922405, Time for Batches: 41.978235
[25472/50000 (51%)], Loss: 1.074813, Time for Batches: 41.954207
[38272/50000 (76%)], Loss: 1.007247, Time for Batches: 41.970640
***** Test set acc: 81.70%, adv: 50.57%.	 Time for an epoch: 257.10
Best result @ 077, 0.5057 

Training Epoch: 78; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.083155, Time for Batches: 41.822787
[25472/50000 (51%)], Loss: 1.125803, Time for Batches: 41.960704
[38272/50000 (76%)], Loss: 1.264038, Time for Batches: 41.918493
- evaluation skipped!	 Time for an epoch: 167.02
Best result @ 077, 0.5057 

Training Epoch: 79; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.008614, Time for Batches: 41.873517
[25472/50000 (51%)], Loss: 0.903607, Time for Batches: 41.952368
[38272/50000 (76%)], Loss: 0.894900, Time for Batches: 41.980695
***** Test set acc: 82.07%, adv: 50.88%.	 Time for an epoch: 257.19
Best result @ 079, 0.5088 

Training Epoch: 80; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.867720, Time for Batches: 41.849571
[25472/50000 (51%)], Loss: 0.948525, Time for Batches: 41.889598
[38272/50000 (76%)], Loss: 1.175407, Time for Batches: 41.894885
- evaluation skipped!	 Time for an epoch: 167.00
Best result @ 079, 0.5088 

Training Epoch: 81; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.011891, Time for Batches: 42.025588
[25472/50000 (51%)], Loss: 0.881120, Time for Batches: 41.916378
[38272/50000 (76%)], Loss: 1.026999, Time for Batches: 41.890360
***** Test set acc: 83.00%, adv: 51.40%.	 Time for an epoch: 257.01
Best result @ 081, 0.514 

Training Epoch: 82; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.977682, Time for Batches: 41.876103
[25472/50000 (51%)], Loss: 0.999544, Time for Batches: 41.903748
[38272/50000 (76%)], Loss: 1.056733, Time for Batches: 41.920283
- evaluation skipped!	 Time for an epoch: 166.99
Best result @ 081, 0.514 

Training Epoch: 83; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.960712, Time for Batches: 41.967046
[25472/50000 (51%)], Loss: 0.918191, Time for Batches: 41.965804
[38272/50000 (76%)], Loss: 0.959185, Time for Batches: 41.827990
***** Test set acc: 83.56%, adv: 51.28%.	 Time for an epoch: 256.78
Best result @ 081, 0.514 

Training Epoch: 84; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.933068, Time for Batches: 41.887722
[25472/50000 (51%)], Loss: 0.880576, Time for Batches: 41.918633
[38272/50000 (76%)], Loss: 0.951541, Time for Batches: 41.878498
- evaluation skipped!	 Time for an epoch: 166.04
Best result @ 081, 0.514 

Training Epoch: 85; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.904048, Time for Batches: 42.016900
[25472/50000 (51%)], Loss: 0.893128, Time for Batches: 41.875676
[38272/50000 (76%)], Loss: 0.936977, Time for Batches: 41.944372
***** Test set acc: 82.44%, adv: 50.26%.	 Time for an epoch: 257.00
Best result @ 081, 0.514 

Training Epoch: 86; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 1.036190, Time for Batches: 41.781024
[25472/50000 (51%)], Loss: 0.906304, Time for Batches: 41.843814
[38272/50000 (76%)], Loss: 0.971542, Time for Batches: 41.902084
- evaluation skipped!	 Time for an epoch: 165.77
Best result @ 081, 0.514 

Training Epoch: 87; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.924071, Time for Batches: 42.057973
[25472/50000 (51%)], Loss: 1.053328, Time for Batches: 41.923702
[38272/50000 (76%)], Loss: 0.890242, Time for Batches: 41.923458
***** Test set acc: 82.34%, adv: 50.84%.	 Time for an epoch: 256.99
Best result @ 081, 0.514 

Training Epoch: 88; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.827913, Time for Batches: 41.852425
[25472/50000 (51%)], Loss: 1.016174, Time for Batches: 41.852173
[38272/50000 (76%)], Loss: 0.979301, Time for Batches: 41.911207
- evaluation skipped!	 Time for an epoch: 165.86
Best result @ 081, 0.514 

Training Epoch: 89; Learning rate: 0.01000000  .....
[12672/50000 (25%)], Loss: 0.844121, Time for Batches: 42.024463
[25472/50000 (51%)], Loss: 0.787637, Time for Batches: 41.914119
[38272/50000 (76%)], Loss: 0.922560, Time for Batches: 41.883158
***** Test set acc: 81.28%, adv: 51.19%.	 Time for an epoch: 257.02
Best result @ 081, 0.514 

Training Epoch: 90; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.800584, Time for Batches: 41.791133
[25472/50000 (51%)], Loss: 0.970704, Time for Batches: 41.932743
[38272/50000 (76%)], Loss: 0.785500, Time for Batches: 41.934665
- evaluation skipped!	 Time for an epoch: 165.88
Best result @ 081, 0.514 

Training Epoch: 91; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.724681, Time for Batches: 42.036687
[25472/50000 (51%)], Loss: 0.762661, Time for Batches: 41.882047
[38272/50000 (76%)], Loss: 0.758131, Time for Batches: 41.917058
***** Test set acc: 83.23%, adv: 51.23%.	 Time for an epoch: 256.96
Best result @ 081, 0.514 

Training Epoch: 92; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.751145, Time for Batches: 41.742820
[25472/50000 (51%)], Loss: 0.895326, Time for Batches: 41.906909
[38272/50000 (76%)], Loss: 0.904734, Time for Batches: 41.909840
- evaluation skipped!	 Time for an epoch: 165.82
Best result @ 081, 0.514 

Training Epoch: 93; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.890863, Time for Batches: 42.089287
[25472/50000 (51%)], Loss: 0.853503, Time for Batches: 41.895401
[38272/50000 (76%)], Loss: 0.900530, Time for Batches: 41.879144
***** Test set acc: 83.18%, adv: 51.38%.	 Time for an epoch: 257.14
Best result @ 081, 0.514 

Training Epoch: 94; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 1.019111, Time for Batches: 41.764261
[25472/50000 (51%)], Loss: 0.897327, Time for Batches: 41.926052
[38272/50000 (76%)], Loss: 0.795356, Time for Batches: 41.893363
- evaluation skipped!	 Time for an epoch: 165.87
Best result @ 081, 0.514 

Training Epoch: 95; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.784534, Time for Batches: 41.987358
[25472/50000 (51%)], Loss: 0.786839, Time for Batches: 41.903593
[38272/50000 (76%)], Loss: 0.745557, Time for Batches: 41.929507
***** Test set acc: 82.99%, adv: 51.15%.	 Time for an epoch: 257.01
Best result @ 081, 0.514 

Training Epoch: 96; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.844792, Time for Batches: 41.814060
[25472/50000 (51%)], Loss: 0.779348, Time for Batches: 41.921641
[38272/50000 (76%)], Loss: 0.860538, Time for Batches: 41.946975
- evaluation skipped!	 Time for an epoch: 165.88
Best result @ 081, 0.514 

Training Epoch: 97; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.736695, Time for Batches: 42.057896
[25472/50000 (51%)], Loss: 0.883126, Time for Batches: 41.918974
[38272/50000 (76%)], Loss: 0.809520, Time for Batches: 41.879569
***** Test set acc: 83.03%, adv: 51.60%.	 Time for an epoch: 256.78
Best result @ 097, 0.516 

Training Epoch: 98; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.815504, Time for Batches: 41.895435
[25472/50000 (51%)], Loss: 0.748053, Time for Batches: 41.842084
[38272/50000 (76%)], Loss: 0.869034, Time for Batches: 41.783002
- evaluation skipped!	 Time for an epoch: 166.79
Best result @ 097, 0.516 

Training Epoch: 99; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.786964, Time for Batches: 41.908115
[25472/50000 (51%)], Loss: 0.810757, Time for Batches: 41.802642
[38272/50000 (76%)], Loss: 0.867185, Time for Batches: 41.791128
***** Test set acc: 82.85%, adv: 51.36%.	 Time for an epoch: 256.34
Best result @ 097, 0.516 

Training Epoch: 100; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 1.037242, Time for Batches: 41.828508
[25472/50000 (51%)], Loss: 0.696644, Time for Batches: 41.754867
[38272/50000 (76%)], Loss: 0.851545, Time for Batches: 41.799760
- evaluation skipped!	 Time for an epoch: 165.49
Best result @ 097, 0.516 

Training Epoch: 101; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.704668, Time for Batches: 41.869301
[25472/50000 (51%)], Loss: 0.892178, Time for Batches: 41.742067
[38272/50000 (76%)], Loss: 0.816913, Time for Batches: 41.716836
***** Test set acc: 82.60%, adv: 51.50%.	 Time for an epoch: 255.98
Best result @ 097, 0.516 

Training Epoch: 102; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.885433, Time for Batches: 41.861587
[25472/50000 (51%)], Loss: 0.942844, Time for Batches: 41.691072
[38272/50000 (76%)], Loss: 0.807081, Time for Batches: 41.730005
- evaluation skipped!	 Time for an epoch: 165.35
Best result @ 097, 0.516 

Training Epoch: 103; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.897767, Time for Batches: 41.893748
[25472/50000 (51%)], Loss: 0.823318, Time for Batches: 41.711686
[38272/50000 (76%)], Loss: 0.923850, Time for Batches: 41.722341
***** Test set acc: 82.72%, adv: 51.57%.	 Time for an epoch: 255.95
Best result @ 097, 0.516 

Training Epoch: 104; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.677104, Time for Batches: 41.857581
[25472/50000 (51%)], Loss: 0.888481, Time for Batches: 41.746981
[38272/50000 (76%)], Loss: 0.715722, Time for Batches: 41.749162
- evaluation skipped!	 Time for an epoch: 165.62
Best result @ 097, 0.516 

Training Epoch: 105; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.760738, Time for Batches: 41.819705
[25472/50000 (51%)], Loss: 0.725812, Time for Batches: 41.694715
[38272/50000 (76%)], Loss: 0.744846, Time for Batches: 41.713856
***** Test set acc: 83.01%, adv: 51.21%.	 Time for an epoch: 255.86
Best result @ 097, 0.516 

Training Epoch: 106; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.791206, Time for Batches: 41.881906
[25472/50000 (51%)], Loss: 0.750645, Time for Batches: 41.757075
[38272/50000 (76%)], Loss: 0.806369, Time for Batches: 41.798254
- evaluation skipped!	 Time for an epoch: 165.47
Best result @ 097, 0.516 

Training Epoch: 107; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.764983, Time for Batches: 41.893064
[25472/50000 (51%)], Loss: 0.830797, Time for Batches: 41.776495
[38272/50000 (76%)], Loss: 0.816463, Time for Batches: 41.732771
***** Test set acc: 82.62%, adv: 50.89%.	 Time for an epoch: 256.13
Best result @ 097, 0.516 

Training Epoch: 108; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.750474, Time for Batches: 41.837431
[25472/50000 (51%)], Loss: 0.808976, Time for Batches: 41.726434
[38272/50000 (76%)], Loss: 0.815408, Time for Batches: 41.769315
- evaluation skipped!	 Time for an epoch: 165.41
Best result @ 097, 0.516 

Training Epoch: 109; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.707291, Time for Batches: 41.838978
[25472/50000 (51%)], Loss: 0.907994, Time for Batches: 41.747814
[38272/50000 (76%)], Loss: 0.853346, Time for Batches: 41.731844
***** Test set acc: 82.86%, adv: 50.99%.	 Time for an epoch: 256.02
Best result @ 097, 0.516 

Training Epoch: 110; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.782629, Time for Batches: 41.865039
[25472/50000 (51%)], Loss: 0.726778, Time for Batches: 41.733583
[38272/50000 (76%)], Loss: 0.666289, Time for Batches: 41.779664
- evaluation skipped!	 Time for an epoch: 165.48
Best result @ 097, 0.516 

Training Epoch: 111; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.785972, Time for Batches: 41.841032
[25472/50000 (51%)], Loss: 0.793314, Time for Batches: 41.719747
[38272/50000 (76%)], Loss: 0.899654, Time for Batches: 41.756140
***** Test set acc: 82.67%, adv: 50.57%.	 Time for an epoch: 255.91
Best result @ 097, 0.516 

Training Epoch: 112; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.820188, Time for Batches: 41.902195
[25472/50000 (51%)], Loss: 0.736455, Time for Batches: 41.708323
[38272/50000 (76%)], Loss: 0.822729, Time for Batches: 41.713738
- evaluation skipped!	 Time for an epoch: 165.39
Best result @ 097, 0.516 

Training Epoch: 113; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 1.045539, Time for Batches: 41.841127
[25472/50000 (51%)], Loss: 0.815842, Time for Batches: 41.735020
[38272/50000 (76%)], Loss: 0.700835, Time for Batches: 41.752682
***** Test set acc: 83.25%, adv: 50.84%.	 Time for an epoch: 256.00
Best result @ 097, 0.516 

Training Epoch: 114; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.789370, Time for Batches: 41.845892
[25472/50000 (51%)], Loss: 0.810891, Time for Batches: 41.753118
[38272/50000 (76%)], Loss: 0.906244, Time for Batches: 41.784808
- evaluation skipped!	 Time for an epoch: 165.71
Best result @ 097, 0.516 

Training Epoch: 115; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.749006, Time for Batches: 41.810631
[25472/50000 (51%)], Loss: 0.779403, Time for Batches: 41.761365
[38272/50000 (76%)], Loss: 0.860468, Time for Batches: 41.768097
***** Test set acc: 82.92%, adv: 51.02%.	 Time for an epoch: 255.99
Best result @ 097, 0.516 

Training Epoch: 116; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.767809, Time for Batches: 41.885186
[25472/50000 (51%)], Loss: 0.681387, Time for Batches: 41.801967
[38272/50000 (76%)], Loss: 0.855739, Time for Batches: 41.757599
- evaluation skipped!	 Time for an epoch: 165.58
Best result @ 097, 0.516 

Training Epoch: 117; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.885398, Time for Batches: 41.827472
[25472/50000 (51%)], Loss: 0.801033, Time for Batches: 41.728986
[38272/50000 (76%)], Loss: 0.956791, Time for Batches: 41.740450
***** Test set acc: 82.43%, adv: 51.10%.	 Time for an epoch: 255.93
Best result @ 097, 0.516 

Training Epoch: 118; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.772395, Time for Batches: 41.914478
[25472/50000 (51%)], Loss: 0.735079, Time for Batches: 41.739840
[38272/50000 (76%)], Loss: 0.846025, Time for Batches: 41.825849
- evaluation skipped!	 Time for an epoch: 165.84
Best result @ 097, 0.516 

Training Epoch: 119; Learning rate: 0.00100000  .....
[12672/50000 (25%)], Loss: 0.736478, Time for Batches: 41.859543
[25472/50000 (51%)], Loss: 0.606992, Time for Batches: 41.762201
[38272/50000 (76%)], Loss: 0.856449, Time for Batches: 41.742695
***** Test set acc: 82.03%, adv: 51.11%.	 Time for an epoch: 255.86
Best result @ 097, 0.516 

