

------ ******* ------ New Log ------ ******* ------
Namespace(GPU_IDs=[0], SEED=0, attack_loss='CE', beta_atk=2, beta_cls=2, checkpoint='./experiments/', exp_path='./experiments/PAT_Res18_cifar10', is_AdvTr=True, is_Train=True, lr=0.1, milestones=[75, 90], momentum=0.9, net_only=True, network='_', resume=False, test_batch_size=250, tr_epochs=120, train_batch_size=128, weight_decay=0.0002)
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer3): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
  )
  (linear): Linear(in_features=512, out_features=10, bias=True)
)
Training Epoch: 0; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.191328, Time for Batches: 39.981130
[25472/50000 (51%)], Loss: 2.137026, Time for Batches: 40.458696
[38272/50000 (76%)], Loss: 2.037686, Time for Batches: 41.212638
***** Test set acc: 34.30%, adv: 24.60%.	 Time for an epoch: 257.18
Best result @ 000, 0.246 

Training Epoch: 1; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.088012, Time for Batches: 41.419167
[25472/50000 (51%)], Loss: 2.090068, Time for Batches: 41.328126
[38272/50000 (76%)], Loss: 1.999809, Time for Batches: 41.314712
- evaluation skipped!	 Time for an epoch: 164.06
Best result @ 000, 0.246 

Training Epoch: 2; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 2.000960, Time for Batches: 41.423066
[25472/50000 (51%)], Loss: 2.068063, Time for Batches: 41.277905
[38272/50000 (76%)], Loss: 1.926873, Time for Batches: 41.268105
- evaluation skipped!	 Time for an epoch: 162.67
Best result @ 000, 0.246 

Training Epoch: 3; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.796461, Time for Batches: 41.385189
[25472/50000 (51%)], Loss: 1.907028, Time for Batches: 41.248839
[38272/50000 (76%)], Loss: 1.827688, Time for Batches: 41.250426
- evaluation skipped!	 Time for an epoch: 163.05
Best result @ 000, 0.246 

Training Epoch: 4; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.847011, Time for Batches: 41.408622
[25472/50000 (51%)], Loss: 1.822466, Time for Batches: 41.244171
[38272/50000 (76%)], Loss: 1.878157, Time for Batches: 41.423185
- evaluation skipped!	 Time for an epoch: 162.77
Best result @ 000, 0.246 

Training Epoch: 5; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.706174, Time for Batches: 41.362219
[25472/50000 (51%)], Loss: 1.761096, Time for Batches: 41.229657
[38272/50000 (76%)], Loss: 1.687093, Time for Batches: 41.235608
- evaluation skipped!	 Time for an epoch: 162.51
Best result @ 000, 0.246 

Training Epoch: 6; Learning rate: 0.10000000  .....
[12672/50000 (25%)], Loss: 1.751976, Time for Batches: 41.357075
